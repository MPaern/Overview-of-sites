# setup -------------------------------------------------------------------
# libraries used in this project
library(tidyverse)
deployment <-  read_csv("data/survey_deployment.csv")
# days deployed -----------------------------------------------------------
class(deployment$`Date and Time Deployed`)
# setup -------------------------------------------------------------------
# libraries used in this project
library(tidyverse)
library(ggplot2)
library(dplyr)
library(suncalc)
library(suntools)
library(lubridate)
library(readr)
# library(esquisse)
# library(ggcorrplot)
maintenance <-  read_csv("data/survey_maintenance.csv")
overview_data <-  read_csv("data/overview_table.csv")
deployment$`Date and Time Deployed` <- as.POSIXct(deployment$`Date and Time Deployed`, format="%m/%d/%Y %H:%M:%S %p")
maintenance$`Date and Time` <- parse_date_time(maintenance$`Date and Time`,
orders = c("m/d/Y I:M:S p", "m/d/Y H:M"))
maintenance$dateretrieved <- ifelse(grepl("etri", maintenance$Comments), maintenance$`Date and Time`, NA)
maintenance$dateretrieved <- as.POSIXct(maintenance$dateretrieved, origin="1970-01-01")
unique_maintenance <- unique(maintenance[, c("Site Name", "dateretrieved")])
unique_maintenance <- na.omit(unique_maintenance)
deployment <- merge(
deployment,
unique_maintenance,
by = "Site Name",
all = TRUE
)
#calculate days between deployment and retrieval
overview_data$`days deployed`<- difftime(deployment$dateretrieved, deployment$`Date and Time Deployed`, units = c())
# distance from water -----------------------------------------------------
overview_data$`distance from water` <- deployment$`Distance from detector to closest water body (m)`[match(overview_data$Location, deployment$`Site Name`)]
View(deployment)
View(overview_data)
site_folders <- list.dirs("data/IDfiles", recursive = FALSE)
site()
site_folders
idfiles_dir <- "data/IDfiles"
# Loop through each site folder
for (site in site_folders) {
# Extract site name
site_name <- basename(site)
# Remove hyphen for variable naming
site_name_clean <- str_replace_all(site_name, "-", "")
# Construct file path using site_name
csv_file <- file.path(idfiles_dir, paste0(site_name, ".csv"))
# Check if file exists before trying to read
if (file.exists(csv_file)) {
# Read the CSV file
input_data <- read_csv(csv_file)
# Dynamically assign it to a variable (e.g., inputCM19)
assign(paste0("input", site_name_clean), input_data, envir = .GlobalEnv)
print(paste("Loaded:", paste0("input", site_name_clean)))
} else {
print(paste("No CSV found for:", site_name))
}
for (site in site_folders) {
# Extract site name
site_name <- basename(site)
# Remove hyphen for variable naming
site_name_clean <- str_replace_all(site_name, "-", "")
# Construct file path using site_name
csv_file <- file.path(idfiles_dir, site_name, "all_id.csv")
# Check if file exists before trying to read
if (file.exists(csv_file)) {
# Read the CSV file
input_data <- read_csv(csv_file)
# Dynamically assign it to a variable (e.g., inputCM19)
assign(paste0("input", site_name_clean), input_data, envir = .GlobalEnv)
print(paste("Loaded:", paste0("input", site_name_clean)))
} else {
print(paste("No CSV found for:", site_name))
}
problems(inputCM01)
problems(input_data)
View(inputCM01)
obs(inputCM01)
lenght(inputCM01)
nrow(inputCM01)
# List all site names
site_names <- sprintf("inputCM%02d", 1:50)
# Initialize row sum
total_rows <- 0
# Loop through each data frame
for (site_var in site_names) {
if (exists(site_var)) {  # Check if the dataframe exists
total_rows <- total_rows + nrow(get(site_var))  # Add row count
} else {
print(paste("Missing:", site_var))
}
# Print total row count
print(paste("Total number of rows across all datasets:", total_rows))
#make column with site name
for (i in 1:50) {
# Generate site variable name dynamically
site_name <- sprintf("CM-%02d", i)  # CM-01, CM-02, ..., CM-50
if (exists(site_names)) {  # Check if dataframe exists
temp_data <- get(site_names)  # Retrieve dataframe
temp_data$Site <- site_name  # Add new column with site name
assign(site_names, temp_data, envir = .GlobalEnv)  # Save back to original variable
} else {
print(paste("Missing:", site_names))  # Debugging message
}
for (i in 1:50) {
# Generate site variable name dynamically
site_var <- sprintf("inputCM%02d", i)  # inputCM01, inputCM02, ..., inputCM50
site_name <- sprintf("CM-%02d", i)  # CM-01, CM-02, ..., CM-50
if (exists(site_var)) {  # Check if dataframe exists
temp_data <- get(site_var)  # Retrieve dataframe
temp_data$Site <- site_name  # Add new column with site name
assign(site_var, temp_data, envir = .GlobalEnv)  # Save back to original variable
} else {
print(paste("Missing:", site_var))  # Debugging message
}
View(inputCM01)
# Retrieve and combine all existing dataframes
all_sites_data <- lapply(site_names, function(site_var) {
if (exists(site_var)) {
get(site_var)  # Retrieve dataframe
}
})
# Bind all dataframes into one large dataframe
cm <- bind_rows(all_sites_data)
# Check the final merged dataframe
dim(cm_data)  # Should return (3463280, X) where X includes the new "Site" column
# Check the final merged dataframe
dim(cm)  # Should return (3463280, X) where X includes the new "Site" column
head(cm)
View(cm)
site_names <- sprintf("inputCM%02d", 1:50)
# Get column names for each dataframe
column_summary <- lapply(site_names, function(site_var) {
if (exists(site_var)) {
colnames(get(site_var))  # Retrieve column names
} else {
NULL
}
})
# Find unique column names across all dataframes
unique_columns <- unique(unlist(column_summary))
print(unique_columns)  # See if any unexpected column appears
for (site_var in site_names) {
if (exists(site_var)) {
df_cols <- colnames(get(site_var))
extra_cols <- setdiff(df_cols, unique_columns[1:45])  # Compare with expected 45 cols
if (length(extra_cols) > 0) {
print(paste("âš  Extra column(s) in", site_var, ":", paste(extra_cols, collapse = ", ")))
}
View(inputCM29)
View(overview_data)
cm01 <- inputCM01 %>%
rename(
filename = "OUT FILE FS",
autoid = "AUTO ID*",
DATE.12 = 'DATE-12',
HOUR.12 = 'HOUR-12',
TIME.12 = 'TIME-12') %>%
mutate(autoid = factor(autoid),
site = factor(site)) %>%
dplyr::select(OUTDIR, FOLDER, filename, DURATION,
DATE, TIME, HOUR,
DATE.12, TIME.12, HOUR.12,
autoid, site
)
barnoise <- ggplot(cm01) +
geom_bar(aes(x= site, fill = autoid), position = "fill") +
scale_fill_manual(name = "AutoID", values = moma.colors("Warhol", n = 13)) +
ylab("Proportion of recordings") +
xlab("Site") +
theme(text = element_text(size = 25))
install.packages((MoMAColors)
install.packages(MoMAColors)
install.packages("devtools")
devtools::install_github("BlakeRMills/MoMAColors")
install.packages("MetBrewer")
barnoise <- ggplot(cm01) +
geom_bar(aes(x= site, fill = autoid), position = "fill") +
scale_fill_manual(name = "AutoID", values = met_brew("Egypt", n = 13)) +
ylab("Proportion of recordings") +
xlab("Site") +
theme(text = element_text(size = 25))
library("MetBrewer")
barnoise <- ggplot(cm01) +
geom_bar(aes(x= site, fill = autoid), position = "fill") +
scale_fill_manual(name = "AutoID", values = met_brew("Egypt", n = 13)) +
ylab("Proportion of recordings") +
xlab("Site") +
theme(text = element_text(size = 25))
barnoise <- ggplot(cm01) +
geom_bar(aes(x= site, fill = autoid), position = "fill") +
scale_fill_manual(name = "AutoID", values = met.brewer("Egypt", n = 13)) +
ylab("Proportion of recordings") +
xlab("Site") +
theme(text = element_text(size = 25))
barnoise
cm01 <- inputCM01 %>%
rename(
filename = "OUT FILE FS",
autoid = "AUTO ID*",
DATE.12 = 'DATE-12',
HOUR.12 = 'HOUR-12',
TIME.12 = 'TIME-12') %>%
mutate(autoid = factor(autoid),
site = factor(site)) %>%
dplyr::select(OUTDIR, FOLDER, filename, DURATION,
DATE, TIME, HOUR,
DATE.12, TIME.12, HOUR.12,
autoid, site
)
# Get a sense for how much noise there is across these sites
## Make a figure illustrating this
barnoise <- ggplot(cm01) +
geom_bar(aes(x= site, fill = autoid), position = "fill") +
scale_fill_manual(name = "AutoID", values = met.brewer("Egypt", n = 13)) +
ylab("Proportion of recordings") +
xlab("Site") +
theme(text = element_text(size = 25))
barnoise
library(esquisse)
esquisser(
)
esquisser(
)
